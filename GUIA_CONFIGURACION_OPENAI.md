# ‚öôÔ∏è Gu√≠a de Configuraci√≥n de OpenAI

Esta gu√≠a explica c√≥mo configurar el modelo de OpenAI, `max_tokens` y gestionar el historial de mensajes para evitar superar los l√≠mites de tokens.

---

## üìã Caracter√≠sticas Implementadas

### 1. **Configuraci√≥n del Modelo**
Puedes elegir qu√© modelo de OpenAI usar (gpt-3.5-turbo, gpt-4, etc.)

### 2. **Configuraci√≥n de max_tokens**
Controla cu√°ntos tokens m√°ximo puede generar la IA en su respuesta

### 3. **Gesti√≥n Inteligente del Historial**
Limita autom√°ticamente el historial de mensajes para no superar los l√≠mites de tokens del modelo

---

## üîß Configuraci√≥n B√°sica

### Variables de Entorno

Crea o edita tu archivo `.env` en la carpeta `backend/`:

```env
# Modelo de OpenAI a usar por defecto
OPENAI_MODEL=gpt-3.5-turbo

# M√°ximo de tokens para las respuestas (por defecto)
OPENAI_MAX_TOKENS=1500

# M√°ximo de tokens para el contexto (por defecto)
OPENAI_MAX_CONTEXT_TOKENS=3000
```

### Modelos Soportados

| Modelo | L√≠mite de Contexto | Uso Recomendado |
|--------|-------------------|-----------------|
| `gpt-3.5-turbo` | 4,096 tokens | Uso general, r√°pido y econ√≥mico |
| `gpt-3.5-turbo-16k` | 16,385 tokens | Contextos m√°s largos, econ√≥mico |
| `gpt-4` | 8,192 tokens | Respuestas m√°s precisas |
| `gpt-4-turbo` | 128,000 tokens | Contextos muy largos |
| `gpt-4o` | 128,000 tokens | Modelo m√°s reciente, mejor rendimiento |
| `gpt-4o-mini` | 128,000 tokens | Versi√≥n optimizada de gpt-4o |

---

## üíª Uso en el C√≥digo

### Funci√≥n Principal: `generar_respuesta_con_chatgpt()`

La funci√≥n ahora acepta par√°metros opcionales para configurar el modelo y max_tokens:

```python
respuesta = generar_respuesta_con_chatgpt(
    pregunta="¬øQu√© hacer en Par√≠s?",
    contexto=contexto_formulario,
    info_clima=info_clima,
    modelo="gpt-4",  # Opcional: especificar modelo
    max_tokens=2000,  # Opcional: especificar max_tokens
    historial=[  # Opcional: incluir historial de conversaci√≥n
        {"role": "user", "content": "Hola"},
        {"role": "assistant", "content": "¬°Hola! ¬øC√≥mo puedo ayudarte?"},
        {"role": "user", "content": "Quiero viajar a Par√≠s"}
    ]
)
```

### Par√°metros:

1. **`modelo`** (opcional):
   - Especifica qu√© modelo usar
   - Si es `None`, usa `OPENAI_MODEL` del `.env` o `"gpt-3.5-turbo"` por defecto

2. **`max_tokens`** (opcional):
   - Limita cu√°ntos tokens puede generar la IA
   - Si es `None`, usa `OPENAI_MAX_TOKENS` del `.env` o `1500` por defecto

3. **`historial`** (opcional):
   - Lista de mensajes anteriores en formato OpenAI
   - Se limitan autom√°ticamente para no exceder el l√≠mite de tokens del modelo

---

## üîç Gesti√≥n Autom√°tica del Historial

### ¬øC√≥mo Funciona?

La funci√≥n `limitar_historial_por_tokens()`:

1. **Mantiene siempre**:
   - El mensaje del sistema (si existe)
   - El mensaje del usuario m√°s reciente

2. **Agrega mensajes anteriores**:
   - Desde el m√°s reciente hasta el m√°s antiguo
   - Hasta alcanzar el l√≠mite de tokens disponible

3. **Calcula tokens disponibles**:
   ```
   Tokens disponibles = L√≠mite del modelo - max_tokens_respuesta - tokens_sistema
   ```

### Ejemplo:

```python
# Historial original (muy largo)
historial = [
    {"role": "user", "content": "Mensaje 1 muy largo..."},
    {"role": "assistant", "content": "Respuesta 1 muy larga..."},
    {"role": "user", "content": "Mensaje 2 muy largo..."},
    {"role": "assistant", "content": "Respuesta 2 muy larga..."},
    {"role": "user", "content": "Mensaje 3 muy largo..."},
    {"role": "assistant", "content": "Respuesta 3 muy larga..."},
    {"role": "user", "content": "Mensaje actual (este s√≠ se incluye siempre)"}
]

# Usando gpt-3.5-turbo (l√≠mite: 4,096 tokens)
# max_tokens: 1,500
# Tokens disponibles para historial: ~2,500

# Resultado: Se incluyen solo los mensajes m√°s recientes que quepan
historial_limitado = limitar_historial_por_tokens(
    mensajes=historial + [{"role": "system", "content": "..."}],
    modelo="gpt-3.5-turbo",
    max_tokens_respuesta=1500
)
```

---

## üìä Estimaci√≥n de Tokens

### Funci√≥n: `estimar_tokens()`

Se usa una aproximaci√≥n simple pero efectiva:

```python
# Aproximaci√≥n: 4 caracteres = 1 token
tokens = len(texto) // 4 + 10  # +10 como buffer de seguridad
```

**Nota**: Esta es una estimaci√≥n. OpenAI usa un tokenizer m√°s sofisticado, pero esta aproximaci√≥n es suficiente para limitar el historial de forma segura.

### Ejemplo:

```python
texto = "Hola, ¬øc√≥mo est√°s?"  # 18 caracteres
tokens_estimados = estimar_tokens(texto)  # ~4-5 tokens
```

---

## ‚úÖ Validaci√≥n de Configuraci√≥n

La funci√≥n `validar_configuracion()` verifica que:

1. El modelo sea v√°lido y est√© soportado
2. `max_tokens` sea mayor que 0
3. `max_tokens` no exceda el l√≠mite del modelo
4. El historial + max_tokens no exceda el l√≠mite del modelo

### Ejemplo de Error:

```python
# Error: max_tokens excede el l√≠mite del modelo
validar_configuracion(
    modelo="gpt-3.5-turbo",  # L√≠mite: 4,096 tokens
    max_tokens=5000  # ‚ùå Mayor que el l√≠mite
)
# Retorna: (False, "max_tokens (5000) no puede ser mayor que el l√≠mite del modelo (4096)")
```

---

## üéØ Ejemplos de Uso

### Ejemplo 1: Usar Modelo Espec√≠fico

```python
respuesta = generar_respuesta_con_chatgpt(
    pregunta="Necesito un itinerario detallado para 7 d√≠as en Tokio",
    modelo="gpt-4",  # Usar GPT-4 para respuestas m√°s precisas
    max_tokens=2500  # Respuestas m√°s largas
)
```

### Ejemplo 2: Incluir Historial de Conversaci√≥n

```python
# Primera pregunta
respuesta1 = generar_respuesta_con_chatgpt(
    pregunta="¬øQu√© hacer en Par√≠s?"
)

# Segunda pregunta (con historial)
respuesta2 = generar_respuesta_con_chatgpt(
    pregunta="¬øY qu√© restaurantes recomiendas?",
    historial=[
        {"role": "user", "content": "¬øQu√© hacer en Par√≠s?"},
        {"role": "assistant", "content": respuesta1}
    ]
)
```

### Ejemplo 3: Configuraci√≥n por Defecto

```python
# Usa la configuraci√≥n del .env o valores por defecto
respuesta = generar_respuesta_con_chatgpt(
    pregunta="¬øCu√°l es el mejor momento para visitar Barcelona?"
)
# Usa: modelo="gpt-3.5-turbo", max_tokens=1500 (o lo configurado en .env)
```

---

## üîß Funciones Disponibles

### `obtener_configuracion_openai(modelo, max_tokens)`

Obtiene la configuraci√≥n con valores por defecto:

```python
config = obtener_configuracion_openai(
    modelo="gpt-4",
    max_tokens=2000
)
# Retorna: {"modelo": "gpt-4", "max_tokens": 2000, "max_context_tokens": 8192}
```

### `limitar_historial_por_tokens(mensajes, modelo, max_tokens_respuesta)`

Limita el historial para que no exceda los l√≠mites:

```python
historial_limitado = limitar_historial_por_tokens(
    mensajes=mensajes_completos,
    modelo="gpt-3.5-turbo",
    max_tokens_respuesta=1500
)
```

### `validar_configuracion(modelo, max_tokens, historial)`

Valida que la configuraci√≥n sea v√°lida:

```python
es_valido, error = validar_configuracion(
    modelo="gpt-3.5-turbo",
    max_tokens=1500,
    historial=mensajes
)
```

### `estimar_tokens(texto)`

Estima el n√∫mero de tokens en un texto:

```python
tokens = estimar_tokens("Hola, ¬øc√≥mo est√°s?")
```

---

## üìù Logging

El sistema registra informaci√≥n √∫til:

```
INFO: Usando modelo: gpt-3.5-turbo, max_tokens: 1500
INFO: Historial limitado: 10 mensajes originales -> 5 mensajes despu√©s del l√≠mite. Tokens estimados: ~2500
```

Esto te ayuda a:
- Ver qu√© configuraci√≥n se est√° usando
- Entender c√≥mo se est√° limitando el historial
- Monitorear el uso de tokens

---

## ‚ö†Ô∏è Consideraciones Importantes

### 1. L√≠mites de Tokens

Cada modelo tiene un l√≠mite de contexto total:

- **gpt-3.5-turbo**: 4,096 tokens
- **gpt-4**: 8,192 tokens
- **gpt-4-turbo**: 128,000 tokens

Este l√≠mite incluye:
- Mensaje del sistema
- Historial de mensajes
- Pregunta actual
- Respuesta generada

### 2. Estimaci√≥n de Tokens

La estimaci√≥n es aproximada:
- **Aproximaci√≥n**: ~4 caracteres = 1 token
- **Precisi√≥n**: Suficiente para limitar el historial de forma segura
- **Buffer**: Se incluye un buffer de seguridad (+10 tokens por mensaje)

### 3. Costos

Modelos m√°s potentes cuestan m√°s:

- **gpt-3.5-turbo**: M√°s econ√≥mico
- **gpt-4**: M√°s caro, pero mejor calidad
- **gpt-4-turbo**: M√°s caro, pero contexto mucho m√°s largo

---

## üéØ Mejores Pr√°cticas

1. **Usa gpt-3.5-turbo para uso general**:
   - R√°pido y econ√≥mico
   - Suficiente para la mayor√≠a de casos

2. **Usa gpt-4 para respuestas cr√≠ticas**:
   - Cuando necesites m√°xima precisi√≥n
   - Para tareas complejas

3. **Configura max_tokens apropiadamente**:
   - Muy bajo: Respuestas cortas o truncadas
   - Muy alto: Puede exceder el l√≠mite o ser costoso

4. **Mant√©n el historial relevante**:
   - El sistema lo limita autom√°ticamente
   - Pero mantener solo lo necesario es m√°s eficiente

---

## üìö Archivos Relacionados

- `backend/openai_config.py` - M√≥dulo de configuraci√≥n
- `backend/main.py` - Uso de la configuraci√≥n
- `backend/.env` - Variables de entorno (crear si no existe)

---

¬°Ahora tienes control total sobre el modelo, tokens y historial! ‚öôÔ∏è

